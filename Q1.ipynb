{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original File Content:\n",
      "<DOC>\n",
      "<DOCNO>\n",
      "1\n",
      "</DOCNO>\n",
      "<TITLE>\n",
      "experimental investigation of the aerodynamics of a\n",
      "wing in a slipstream .\n",
      "</TITLE>\n",
      "<AUTHOR>\n",
      "brenckman,m.\n",
      "</AUTHOR>\n",
      "<BIBLIO>\n",
      "j. ae. scs. 25, 1958, 324.\n",
      "</BIBLIO>\n",
      "<TEXT>\n",
      "  an experimental study of a wing in a propeller slipstream was\n",
      "made in order to determine the spanwise distribution of the lift\n",
      "increase due to slipstream at different angles of attack of the wing\n",
      "and at different free stream to slipstream velocity ratios .  the\n",
      "results were intended in part as an evaluation basis for different\n",
      "theoretical treatments of this problem .\n",
      "  the comparative span loading curves, together with supporting\n",
      "evidence, showed that a substantial part of the lift increment\n",
      "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
      "effect .  the integrated remaining lift increment,\n",
      "after subtracting this destalling lift, was found to agree\n",
      "well with a potential flow theory .\n",
      "  an empirical evaluation of the destalling effects was made for\n",
      "the specific configuration of the experiment .\n",
      "</TEXT>\n",
      "</DOC>\n",
      "\n",
      "***************\n",
      "Raw Extracted Data:\n",
      " experimental investigation of the aerodynamics of a wing in a slipstream .     an experimental study of a wing in a propeller slipstream was made in order to determine the spanwise distribution of the lift increase due to slipstream at different angles of attack of the wing and at different free stream to slipstream velocity ratios .  the results were intended in part as an evaluation basis for different theoretical treatments of this problem .   the comparative span loading curves, together with supporting evidence, showed that a substantial part of the lift increment produced by the slipstream was due to a /destalling/ or boundary layer control effect .  the integrated remaining lift increment, after subtracting this destalling lift, was found to agree well with a potential flow theory .   an empirical evaluation of the destalling effects was made for the specific configuration of the experiment . \n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Original File Content:\n",
      "<DOC>\n",
      "<DOCNO>\n",
      "2\n",
      "</DOCNO>\n",
      "<TITLE>\n",
      "simple shear flow past a flat plate in an incompressible fluid of small\n",
      "viscosity .\n",
      "</TITLE>\n",
      "<AUTHOR>\n",
      "ting-yili\n",
      "</AUTHOR>\n",
      "<BIBLIO>\n",
      "department of aeronautical engineering, rensselaer polytechnic\n",
      "institute\n",
      "troy, n.y.\n",
      "</BIBLIO>\n",
      "<TEXT>\n",
      "in the study of high-speed viscous flow past a two-dimensional body it\n",
      "is usually necessary to consider a curved shock wave emitting from the\n",
      "nose or leading edge of the body .  consequently, there exists an inviscid\n",
      "rotational flow region between the shock wave and the boundary layer\n",
      ".  such a situation arises, for instance, in the study of the hypersonic\n",
      "viscous flow past a flat plate .  the situation is somewhat different\n",
      "from prandtl's classical boundary-layer problem . in prandtl's\n",
      "original problem the inviscid free stream outside the boundary layer is\n",
      "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
      "free stream must be considered as rotational .  the possible effects of\n",
      "vorticity have been recently discussed by ferri and libby .  in the present\n",
      "paper, the simple shear flow past a flat plate in a fluid of small\n",
      "viscosity is investigated .  it can be shown that this problem can again\n",
      "be treated by the boundary-layer approximation, the only novel feature\n",
      "being that the free stream has a constant vorticity .  the discussion\n",
      "here is restricted to two-dimensional incompressible steady flow .\n",
      "</TEXT>\n",
      "</DOC>\n",
      "\n",
      "***************\n",
      "Raw Extracted Data:\n",
      " simple shear flow past a flat plate in an incompressible fluid of small viscosity .   in the study of high speed viscous flow past a two dimensional body it is usually necessary to consider a curved shock wave emitting from the nose or leading edge of the body .  consequently, there exists an inviscid rotational flow region between the shock wave and the boundary layer .  such a situation arises, for instance, in the study of the hypersonic viscous flow past a flat plate .  the situation is somewhat different from prandtl's classical boundary layer problem . in prandtl's original problem the inviscid free stream outside the boundary layer is irrotational while in a hypersonic boundary layer problem the inviscid free stream must be considered as rotational .  the possible effects of vorticity have been recently discussed by ferri and libby .  in the present paper, the simple shear flow past a flat plate in a fluid of small viscosity is investigated .  it can be shown that this problem can again be treated by the boundary layer approximation, the only novel feature being that the free stream has a constant vorticity .  the discussion here is restricted to two dimensional incompressible steady flow . \n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Original File Content:\n",
      "<DOC>\n",
      "<DOCNO>\n",
      "3\n",
      "</DOCNO>\n",
      "<TITLE>\n",
      "the boundary layer in simple shear flow past a flat plate .\n",
      "</TITLE>\n",
      "<AUTHOR>\n",
      "m. b. glauert\n",
      "</AUTHOR>\n",
      "<BIBLIO>\n",
      "department of mathematics, university of manchester, manchester,\n",
      "england\n",
      "</BIBLIO>\n",
      "<TEXT>\n",
      "the boundary-layer equations are presented for steady\n",
      "incompressible flow with no pressure gradient .\n",
      "</TEXT>\n",
      "</DOC>\n",
      "\n",
      "***************\n",
      "Raw Extracted Data:\n",
      " the boundary layer in simple shear flow past a flat plate .   the boundary layer equations are presented for steady incompressible flow with no pressure gradient . \n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Original File Content:\n",
      "<DOC>\n",
      "<DOCNO>\n",
      "4\n",
      "</DOCNO>\n",
      "<TITLE>\n",
      "approximate solutions of the incompressible laminar\n",
      "boundary layer equations for a plate in shear flow .\n",
      "</TITLE>\n",
      "<AUTHOR>\n",
      "yen,k.t.\n",
      "</AUTHOR>\n",
      "<BIBLIO>\n",
      "j. ae. scs. 22, 1955, 728.\n",
      "</BIBLIO>\n",
      "<TEXT>\n",
      "  the two-dimensional steady boundary-layer\n",
      "problem for a flat plate in a\n",
      "shear flow of incompressible fluid is considered .\n",
      "solutions for the boundarylayer\n",
      "thickness, skin friction, and the velocity\n",
      "distribution in the boundary\n",
      "layer are obtained by the karman-pohlhausen\n",
      "technique .  comparison with\n",
      "the boundary layer of a uniform flow has also\n",
      "been made to show the effect of\n",
      "vorticity .\n",
      "</TEXT>\n",
      "</DOC>\n",
      "\n",
      "***************\n",
      "Raw Extracted Data:\n",
      " approximate solutions of the incompressible laminar boundary layer equations for a plate in shear flow .     the two dimensional steady boundary layer problem for a flat plate in a shear flow of incompressible fluid is considered . solutions for the boundarylayer thickness, skin friction, and the velocity distribution in the boundary layer are obtained by the karman pohlhausen technique .  comparison with the boundary layer of a uniform flow has also been made to show the effect of vorticity . \n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Original File Content:\n",
      "<DOC>\n",
      "<DOCNO>\n",
      "5\n",
      "</DOCNO>\n",
      "<TITLE>\n",
      "one-dimensional transient heat conduction into a double-layer\n",
      "slab subjected to a linear heat input for a small time\n",
      "internal .\n",
      "</TITLE>\n",
      "<AUTHOR>\n",
      "wasserman,b.\n",
      "</AUTHOR>\n",
      "<BIBLIO>\n",
      "j. ae. scs. 24, 1957, 924.\n",
      "</BIBLIO>\n",
      "<TEXT>\n",
      "  analytic solutions are presented for the transient heat conduction\n",
      "in composite slabs exposed at one surface to a\n",
      "triangular heat rate .  this type of heating rate may occur, for\n",
      "example, during aerodynamic heating .\n",
      "</TEXT>\n",
      "</DOC>\n",
      "\n",
      "***************\n",
      "Raw Extracted Data:\n",
      " one dimensional transient heat conduction into a double layer slab subjected to a linear heat input for a small time internal .     analytic solutions are presented for the transient heat conduction in composite slabs exposed at one surface to a triangular heat rate .  this type of heating rate may occur, for example, during aerodynamic heating . \n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download(\"stopwords\",quiet=True)\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "\n",
    "def extract_text(s,find_tags = True, remove_separators = True):\n",
    "    new_s = s[::]\n",
    "    if(find_tags):\n",
    "        start = [0,0] #Start is inclusive \n",
    "        end = [len(new_s)+1,0] #End is exclusive\n",
    "        for i in range(len(s)):\n",
    "            if(i + 7 <= len(s) and s[i:i+7] == \"<TITLE>\"):\n",
    "                start[0] = i+7\n",
    "            elif(i + 8 <= len(s) and s[i:i+8] == \"</TITLE>\"):\n",
    "                end[0] = i\n",
    "            elif(i + 6 <= len(s) and s[i:i+6] == \"<TEXT>\"):\n",
    "                start[1] = i+6\n",
    "            elif(i + 7 <= len(s) and s[i:i+7] == \"</TEXT>\"):\n",
    "                end[1] = i\n",
    "        new_s = s[start[0]:end[0]] + \" \" + s[start[1]:end[1]]\n",
    "    if(remove_separators):\n",
    "        new_s = \" \".join(new_s.split(\"\\n\"))\n",
    "        new_s = \" \".join(new_s.split(\"-\"))\n",
    "    return new_s\n",
    "\n",
    "def pre_process_text(new_s, remove_duplicates = False):\n",
    "    new_s = new_s.lower()\n",
    "    translate_table = dict((ord(char), \" \") for char in string.punctuation)   \n",
    "    new_s = new_s.translate(translate_table)\n",
    "    li = word_tokenize(new_s)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filter_li = []\n",
    "    for words in li:\n",
    "        if(words not in stop_words):\n",
    "            filter_li.append(words)\n",
    "    if(remove_duplicates):\n",
    "        filter_li = list(set(filter_li))\n",
    "    return filter_li\n",
    "\n",
    "def getNum(i):\n",
    "    return (4-len(str(i)))*\"0\" + str(i)\n",
    "\n",
    "def get_vocabulary(dataset):\n",
    "    for i in dataset:\n",
    "        for j in i.split():\n",
    "            if(j not in vocab):\n",
    "                vocab.append(j)\n",
    "\n",
    "#------------------------------------------#### MAIN ####------------------------------------------#\n",
    "overwrite_file = True\n",
    "sample_number = 0\n",
    "vocab = []\n",
    "doc_list = []\n",
    "id = 0\n",
    "\n",
    "for j in range(1,1401): # 1,1401\n",
    "    file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(j),\"r\")\n",
    "    file_content = file.read()\n",
    "    file.close()\n",
    "    extracted_content = extract_text(file_content,find_tags=True,remove_separators=True)\n",
    "    if(sample_number < 5):\n",
    "        sample_number+=1\n",
    "        print(\"Original File Content:\")\n",
    "        print(file_content)\n",
    "        print(\"***************\")\n",
    "        print(\"Raw Extracted Data:\")\n",
    "        print(extracted_content)\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    if(overwrite_file):\n",
    "        file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(j),\"w\")\n",
    "        file.write(extracted_content)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "total_documents = 0\n",
    "for i in range(1,1401):\n",
    "    total_documents = total_documents+1\n",
    "    file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(i),\"r\")\n",
    "    file_content = file.read()\n",
    "    file.close()\n",
    "    extracted_content = pre_process_text(file_content, remove_duplicates=False)\n",
    "    get_vocabulary(extracted_content)\n",
    "    if(sample_number < 5):\n",
    "        sample_number+=1\n",
    "        print(\"Before Preprocessing:\")\n",
    "        print(file_content)\n",
    "        print(\"***************\")\n",
    "        print(\"After Preprocessing: (Comma Separated Tokens)\")\n",
    "        for k in extracted_content:\n",
    "            print(k,end=\", \")\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Scheme time: 16.81926131248474\n",
      "Raw Count Scheme time: 16.734785556793213\n",
      "Term Frequency Scheme time: 27.17898678779602\n",
      "Log Normalization Scheme time: 26.030656576156616\n",
      "Double Normalization Scheme time: 19.114404916763306\n",
      "----------------------------------------SCHEME:Binary-------------------------------------------\n",
      "Document:  0677 , Tfidf score:  6.698194669434006\n",
      "Document:  0946 , Tfidf score:  6.300055546427604\n",
      "Document:  1366 , Tfidf score:  6.240245450316783\n",
      "Document:  0815 , Tfidf score:  5.887458271855376\n",
      "Document:  0001 , Tfidf score:  5.708712778454509\n",
      "\n",
      "----------------------------------------SCHEME:Raw Count-------------------------------------------\n",
      "Document:  1291 , Tfidf score:  15.517378538045824\n",
      "Document:  0262 , Tfidf score:  13.705624037750864\n",
      "Document:  1344 , Tfidf score:  11.7001008939\n",
      "Document:  0156 , Tfidf score:  10.233549053655969\n",
      "Document:  0673 , Tfidf score:  9.240419922840783\n",
      "\n",
      "----------------------------------------SCHEME:Term Frequency-------------------------------------------\n",
      "Document:  0339 , Tfidf score:  0.015275264486399494\n",
      "Document:  0001 , Tfidf score:  0.013032931027203584\n",
      "Document:  0250 , Tfidf score:  0.012925077508169387\n",
      "Document:  0507 , Tfidf score:  0.012859525992637686\n",
      "Document:  1291 , Tfidf score:  0.010744626578145886\n",
      "\n",
      "----------------------------------------SCHEME:Log Nomralization-------------------------------------------\n",
      "Document:  0262 , Tfidf score:  0.7815932580248388\n",
      "Document:  0001 , Tfidf score:  0.7011927395360749\n",
      "Document:  0677 , Tfidf score:  0.6382207348452611\n",
      "Document:  0197 , Tfidf score:  0.5886093650898518\n",
      "Document:  0277 , Tfidf score:  0.5816627941734535\n",
      "\n",
      "----------------------------------------SCHEME:Double Nomralization-------------------------------------------\n",
      "Document:  0718 , Tfidf score:  11858.341659880414\n",
      "Document:  0212 , Tfidf score:  11845.372317222784\n",
      "Document:  1088 , Tfidf score:  11840.44259553386\n",
      "Document:  0262 , Tfidf score:  11840.391207934832\n",
      "Document:  0489 , Tfidf score:  11840.359067470654\n"
     ]
    }
   ],
   "source": [
    "#TFIDF MATRIX\n",
    "import time\n",
    "\n",
    "def getNum(i):\n",
    "    return (4-len(str(i)))*\"0\" + str(i)\n",
    "\n",
    "def get_vocabulary(dataset):\n",
    "    for i in dataset:\n",
    "        for j in i.split():\n",
    "            if(j not in vocab):\n",
    "                vocab.append(j)\n",
    "\n",
    "def get_matrix():\n",
    "    matrix = {}\n",
    "    for i in range(1,1401):\n",
    "        newlist = {}\n",
    "        for k in range(len(vocab)):\n",
    "            newlist[vocab[k]] = 0\n",
    "        matrix[getNum(i)] = newlist.copy()\n",
    "    matrix[\"num_docs\"] = newlist.copy()\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def binary_tfidf():\n",
    "    start = time.time()\n",
    "    for i in range(1,1401):\n",
    "        file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(i),\"r\")\n",
    "        file_content = file.read()\n",
    "        file.close()\n",
    "        extracted_content = pre_process_text(file_content, remove_duplicates=False)\n",
    "        for j in extracted_content:\n",
    "            matrix_binary[getNum(i)][j] = 1\n",
    "\n",
    "    for word in vocab:\n",
    "        shu = 0\n",
    "        lst = []\n",
    "        for i in range(1,1401):\n",
    "            if (matrix_binary[getNum(i)][word] > 0):\n",
    "                shu = shu+1\n",
    "                if getNum(i) not in lst:\n",
    "                    lst.append(getNum(i))\n",
    "        postinglist_binary[word] = lst    \n",
    "        matrix_binary[\"num_docs\"][word] = shu\n",
    "\n",
    "    for i in matrix_binary:\n",
    "        for j in matrix_binary[i]:\n",
    "            tf_value = matrix_binary[i][j]\n",
    "            idf_value = np.log10(total_documents/(matrix_binary[\"num_docs\"][j]+1))\n",
    "            tf_idf_value = tf_value*idf_value\n",
    "            matrix_binary[i][j] = tf_idf_value\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Binary Scheme time:\",end-start)\n",
    "\n",
    "\n",
    "def rawcount_tfidf():\n",
    "    start = time.time()\n",
    "    for i in range(1,1401):\n",
    "        file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(i),\"r\")\n",
    "        file_content = file.read()\n",
    "        file.close()\n",
    "        extracted_content = pre_process_text(file_content, remove_duplicates=False)\n",
    "        for j in extracted_content:\n",
    "            matrix_raw[getNum(i)][j] += 1\n",
    "\n",
    "    for word in vocab:\n",
    "        shu = 0\n",
    "        lst = []\n",
    "        for i in range(1,1401):\n",
    "            if (matrix_raw[getNum(i)][word] > 0):\n",
    "                shu = shu+1\n",
    "                if getNum(i) not in lst:\n",
    "                    lst.append(getNum(i))\n",
    "        postinglist_raw[word] = lst  \n",
    "        matrix_raw[\"num_docs\"][word] = shu\n",
    "\n",
    "    for i in matrix_raw:\n",
    "        for j in matrix_raw[i]:\n",
    "            tf_value = matrix_raw[i][j]\n",
    "            idf_value = np.log10(total_documents/(matrix_raw[\"num_docs\"][j]+1))\n",
    "            tf_idf_value = tf_value*idf_value\n",
    "            matrix_raw[i][j] = tf_idf_value\n",
    "    end = time.time()\n",
    "    print(\"Raw Count Scheme time:\",end-start)\n",
    "\n",
    "\n",
    "def termfreq_tfidf():\n",
    "    start = time.time()\n",
    "    for i in range(1,1401):\n",
    "        file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(i),\"r\")\n",
    "        file_content = file.read()\n",
    "        file.close()\n",
    "        extracted_content = pre_process_text(file_content, remove_duplicates=False)\n",
    "        total_terms = 0\n",
    "        for j in extracted_content:\n",
    "            total_terms += 1\n",
    "            matrix_termfreq[getNum(i)][j] += 1\n",
    "        for j in matrix_termfreq[getNum(i)]:\n",
    "            ftd = matrix_termfreq[getNum(i)][j]\n",
    "            matrix_termfreq[getNum(i)][j] = (ftd)/(total_terms-ftd+1)\n",
    "\n",
    "    for word in vocab:\n",
    "        shu = 0\n",
    "        lst = []\n",
    "        for i in range(1,1401):\n",
    "            if (matrix_termfreq[getNum(i)][word] > 0):\n",
    "                shu = shu+1\n",
    "                lst.append(getNum(i))\n",
    "        postinglist_termfreq[word] = lst  \n",
    "        matrix_termfreq[\"num_docs\"][word] = shu\n",
    "\n",
    "    for i in matrix_termfreq:\n",
    "        for j in matrix_termfreq[i]:\n",
    "            tf_value = matrix_termfreq[i][j]\n",
    "            idf_value = np.log10(total_documents/(matrix_termfreq[\"num_docs\"][j]+1))\n",
    "            tf_idf_value = tf_value*idf_value\n",
    "            matrix_termfreq[i][j] = tf_idf_value\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Term Frequency Scheme time:\",end-start)       \n",
    "\n",
    "def lognorm_tfidf():\n",
    "    start = time.time()\n",
    "    for i in range(1,1401):\n",
    "        file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(i),\"r\")\n",
    "        file_content = file.read()\n",
    "        file.close()\n",
    "        extracted_content = pre_process_text(file_content, remove_duplicates=False)\n",
    "        for j in extracted_content:\n",
    "            matrix_lognorm[getNum(i)][j] += 1\n",
    "\n",
    "    for word in vocab:\n",
    "        shu = 0\n",
    "        lst = []\n",
    "        for i in range(1,1401):\n",
    "            if (matrix_lognorm[getNum(i)][word] > 0):\n",
    "                shu = shu+1\n",
    "                lst.append(getNum(i))\n",
    "        postinglist_lognorm[word] = lst \n",
    "        matrix_lognorm[\"num_docs\"][word] = shu\n",
    "\n",
    "    for i in matrix_lognorm:\n",
    "        if(i!=\"num_docs\"):\n",
    "            for j in matrix_lognorm[i]:\n",
    "                ftd = matrix_lognorm[i][j]\n",
    "                matrix_lognorm[i][j] = np.log10(1+ftd)\n",
    "\n",
    "\n",
    "    for i in matrix_lognorm:\n",
    "        for j in matrix_lognorm[i]:\n",
    "            tf_value = matrix_lognorm[i][j]\n",
    "            idf_value = np.log10(total_documents/(matrix_lognorm[\"num_docs\"][j]+1))\n",
    "            tf_idf_value = tf_value*idf_value\n",
    "            matrix_lognorm[i][j] = tf_idf_value\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Log Normalization Scheme time:\",end-start)\n",
    "\n",
    "\n",
    "def double_lognorm_tfidf():\n",
    "    start = time.time()\n",
    "    for i in range(1,1401):\n",
    "        file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(i),\"r\")\n",
    "        file_content = file.read()\n",
    "        file.close()\n",
    "        extracted_content = pre_process_text(file_content, remove_duplicates=False)\n",
    "        for j in extracted_content:\n",
    "            matrix_doublenorm[getNum(i)][j] += 1\n",
    "    for word in vocab:\n",
    "        shu = 0\n",
    "        lst = []\n",
    "        for i in range(1,1401):\n",
    "            if (matrix_doublenorm[getNum(i)][word] > 0):\n",
    "                shu = shu+1\n",
    "                lst.append(getNum(i))\n",
    "        postinglist_doublenorm[word] = lst \n",
    "        matrix_doublenorm[\"num_docs\"][word] = shu\n",
    "    for i in matrix_doublenorm:\n",
    "        if(i!=\"num_docs\"):\n",
    "            max_vals=matrix_doublenorm[i].values()\n",
    "            l=sorted(max_vals)\n",
    "            max1 = l[-1]\n",
    "            max2 = l[-2]\n",
    "            if(max2 == 0):\n",
    "                max2 = max1\n",
    "            for j in matrix_doublenorm[i]:\n",
    "                ftd = matrix_doublenorm[i][j]\n",
    "                if(ftd == max1):\n",
    "                    matrix_doublenorm[i][j] = 0.5+0.5*(ftd/max2)\n",
    "                else:\n",
    "                    matrix_doublenorm[i][j] = 0.5+0.5*(ftd/max1)\n",
    "    for i in matrix_doublenorm:\n",
    "        for j in matrix_doublenorm[i]:\n",
    "            tf_value = matrix_doublenorm[i][j]\n",
    "            idf_value = np.log10(total_documents/(matrix_doublenorm[\"num_docs\"][j]+1))\n",
    "            tf_idf_value = tf_value*idf_value\n",
    "            matrix_doublenorm[i][j] = tf_idf_value\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Double Normalization Scheme time:\",end-start)\n",
    "\n",
    "\n",
    "def tfidf_score_binary(query):\n",
    "    query_dict_tf = {}\n",
    "    newlist = {}\n",
    "    for k in range(len(vocab)):\n",
    "        newlist[vocab[k]] = 0\n",
    "    query_dict_tf = newlist.copy()\n",
    "\n",
    "    for i in query_dict_tf:\n",
    "        if i in query:\n",
    "            query_dict_tf[i] = 1\n",
    "    for i in query_dict_tf:\n",
    "        tf_val = query_dict_tf[i]\n",
    "        idf_val = np.log10(total_documents/(len(postinglist_binary[i])+1))\n",
    "        tfidf_val = tf_val*idf_val\n",
    "        query_dict_tf[i] = tfidf_val\n",
    "    query_tf = []\n",
    "    for i in query_dict_tf:\n",
    "       query_tf.append(query_dict_tf[i])  \n",
    "    query_tfidf_vector = np.array(query_tf)\n",
    "    tfidf_score_binary = {} \n",
    "    for i in matrix_binary:\n",
    "        if(i != \"num_docs\"):\n",
    "            doc = []\n",
    "            for j in matrix_binary[i]:\n",
    "                doc.append(matrix_binary[i][j])\n",
    "            doc_tfidf_vector =  np.array(doc)\n",
    "            tfidf_score = np.dot(query_tfidf_vector,doc_tfidf_vector)\n",
    "        tfidf_score_binary[i] = tfidf_score    \n",
    "    sorted_tfidf_score_binary = dict( sorted(tfidf_score_binary.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ctr=0\n",
    "    print(\"----------------------------------------SCHEME:Binary-------------------------------------------\")\n",
    "    for i in sorted_tfidf_score_binary:\n",
    "        print(\"Document: \", i , \", Tfidf score: \",sorted_tfidf_score_binary[i])  \n",
    "        ctr += 1\n",
    "        if ctr == 5:\n",
    "            break      \n",
    "\n",
    "def tfidf_score_raw(query):\n",
    "    query_dict_tf = {}\n",
    "    newlist = {}\n",
    "    for k in range(len(vocab)):\n",
    "        newlist[vocab[k]] = 0\n",
    "    query_dict_tf = newlist.copy()\n",
    "\n",
    "    for i in query_dict_tf:\n",
    "        if i in query:\n",
    "            query_dict_tf[i] += 1\n",
    "    for i in query_dict_tf:\n",
    "        tf_val = query_dict_tf[i]\n",
    "        idf_val = np.log10(total_documents/(len(postinglist_raw[i])+1))\n",
    "        tfidf_val = tf_val*idf_val\n",
    "        query_dict_tf[i] = tfidf_val\n",
    "    query_tf = []\n",
    "    for i in query_dict_tf:\n",
    "       query_tf.append(query_dict_tf[i])  \n",
    "    query_tfidf_vector = np.array(query_tf)\n",
    "    tfidf_score_raw = {}    \n",
    "    for i in matrix_raw:\n",
    "        if(i != \"num_docs\"):\n",
    "            doc = []\n",
    "            for j in matrix_raw[i]:\n",
    "                doc.append(matrix_raw[i][j])\n",
    "            doc_tfidf_vector =  np.array(doc)\n",
    "            tfidf_score = np.dot(query_tfidf_vector,doc_tfidf_vector)\n",
    "        tfidf_score_raw[i] = tfidf_score    \n",
    "    sorted_tfidf_score_raw = dict( sorted(tfidf_score_raw.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ctr=0\n",
    "    print()\n",
    "    print(\"----------------------------------------SCHEME:Raw Count-------------------------------------------\")\n",
    "    for i in sorted_tfidf_score_raw:\n",
    "        print(\"Document: \", i , \", Tfidf score: \",sorted_tfidf_score_raw[i])  \n",
    "        ctr += 1\n",
    "        if ctr == 5:\n",
    "            break \n",
    "\n",
    "def tfidf_score_termfreq(query):\n",
    "    query_dict_tf = {}\n",
    "    newlist = {}\n",
    "    for k in range(len(vocab)):\n",
    "        newlist[vocab[k]] = 0\n",
    "    query_dict_tf = newlist.copy()\n",
    "    total_terms_query = 0\n",
    "    for i in query_dict_tf:\n",
    "        if i in query:\n",
    "            total_terms_query += 1\n",
    "            query_dict_tf[i] += 1\n",
    "    for i in query_dict_tf:\n",
    "        ftdq = query_dict_tf[i]\n",
    "        query_dict_tf[i] = ftdq/(total_terms_query-ftdq)\n",
    "    for i in query_dict_tf:\n",
    "        tf_val = query_dict_tf[i]\n",
    "        idf_val = np.log10(total_documents/(len(postinglist_termfreq[i])+1))\n",
    "        tfidf_val = tf_val*idf_val\n",
    "        query_dict_tf[i] = tfidf_val\n",
    "    query_tf = []\n",
    "    for i in query_dict_tf:\n",
    "       query_tf.append(query_dict_tf[i])  \n",
    "    query_tfidf_vector = np.array(query_tf)\n",
    "    tfidf_score_termfreq = {}    \n",
    "    for i in matrix_termfreq:\n",
    "        if(i != \"num_docs\"):\n",
    "            doc = []\n",
    "            for j in matrix_termfreq[i]:\n",
    "                doc.append(matrix_termfreq[i][j])\n",
    "            doc_tfidf_vector =  np.array(doc)\n",
    "            tfidf_score = np.dot(query_tfidf_vector,doc_tfidf_vector)\n",
    "        tfidf_score_termfreq[i] = tfidf_score    \n",
    "    sorted_tfidf_score_termfreq = dict( sorted(tfidf_score_termfreq.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ctr=0\n",
    "    print()\n",
    "    print(\"----------------------------------------SCHEME:Term Frequency-------------------------------------------\")\n",
    "    for i in sorted_tfidf_score_termfreq:\n",
    "        print(\"Document: \", i , \", Tfidf score: \",sorted_tfidf_score_termfreq[i])  \n",
    "        ctr += 1\n",
    "        if ctr == 5:\n",
    "            break \n",
    "\n",
    "def tfidf_score_lognorm(query):\n",
    "    query_dict_tf = {}\n",
    "    newlist = {}\n",
    "    for k in range(len(vocab)):\n",
    "        newlist[vocab[k]] = 0\n",
    "    query_dict_tf = newlist.copy()\n",
    "    for i in query_dict_tf:\n",
    "        if i in query:\n",
    "            query_dict_tf[i] += 1\n",
    "    for i in query_dict_tf:\n",
    "        ftdq = query_dict_tf[i]\n",
    "        query_dict_tf[i] = np.log10(ftdq+1)\n",
    "    for i in query_dict_tf:\n",
    "        tf_val = query_dict_tf[i]\n",
    "        idf_val = np.log10(total_documents/(len(postinglist_lognorm[i])+1))\n",
    "        tfidf_val = tf_val*idf_val\n",
    "        query_dict_tf[i] = tfidf_val\n",
    "    query_tf = []\n",
    "    for i in query_dict_tf:\n",
    "       query_tf.append(query_dict_tf[i])  \n",
    "    query_tfidf_vector = np.array(query_tf)\n",
    "    tfidf_score_lognorm = {}    \n",
    "    for i in matrix_lognorm :\n",
    "        if(i != \"num_docs\"):\n",
    "            doc = []\n",
    "            for j in matrix_lognorm[i]:\n",
    "                doc.append(matrix_lognorm[i][j])\n",
    "            doc_tfidf_vector =  np.array(doc)\n",
    "            tfidf_score = np.dot(query_tfidf_vector,doc_tfidf_vector)\n",
    "        tfidf_score_lognorm [i] = tfidf_score    \n",
    "    sorted_tfidf_score_lognorm  = dict( sorted(tfidf_score_lognorm .items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ctr=0\n",
    "    print()\n",
    "    print(\"----------------------------------------SCHEME:Log Nomralization-------------------------------------------\")\n",
    "    for i in sorted_tfidf_score_lognorm :\n",
    "        print(\"Document: \", i , \", Tfidf score: \",sorted_tfidf_score_lognorm [i])  \n",
    "        ctr += 1\n",
    "        if ctr == 5:\n",
    "            break \n",
    "\n",
    "\n",
    "def tfidf_score_doublenorm(query):\n",
    "    query_dict_tf = {}\n",
    "    newlist = {}\n",
    "    for k in range(len(vocab)):\n",
    "        newlist[vocab[k]] = 0\n",
    "    query_dict_tf = newlist.copy()\n",
    "    for i in query_dict_tf:\n",
    "        if i in query:\n",
    "            query_dict_tf[i] += 1\n",
    "    max_vals=query_dict_tf.values()\n",
    "    l=sorted(max_vals)\n",
    "    max1 = l[-1]\n",
    "    max2 = l[-2]\n",
    "    if(max2 == 0):\n",
    "        max2 = max1\n",
    "    for i in query_dict_tf:\n",
    "        ftdq = query_dict_tf[i]\n",
    "        if(ftdq == max1):\n",
    "            query_dict_tf[i] = 0.5+0.5*(ftdq/max2)\n",
    "        else:\n",
    "            query_dict_tf[i] = 0.5+0.5*(ftdq/max1)\n",
    "    for i in query_dict_tf:\n",
    "        tf_val = query_dict_tf[i]\n",
    "        idf_val =  np.log10(total_documents/(len(postinglist_doublenorm[i])+1))\n",
    "        tfidf_val = tf_val*idf_val\n",
    "        query_dict_tf[i] = tfidf_val\n",
    "\n",
    "    query_tf = []\n",
    "    for i in query_dict_tf:\n",
    "       query_tf.append(query_dict_tf[i])  \n",
    "    query_tfidf_vector = np.array(query_tf)\n",
    "    tfidf_score_doublenorm = {}    \n",
    "    for i in matrix_doublenorm:\n",
    "        if(i != \"num_docs\"):\n",
    "            doc = []\n",
    "            for j in matrix_doublenorm[i]:\n",
    "                doc.append(matrix_doublenorm[i][j])\n",
    "            doc_tfidf_vector =  np.array(doc)\n",
    "            tfidf_score = np.dot(query_tfidf_vector,doc_tfidf_vector)\n",
    "        tfidf_score_doublenorm[i] = tfidf_score    \n",
    "    sorted_tfidf_score_doublenorm = dict( sorted(tfidf_score_doublenorm.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ctr=0\n",
    "    print()\n",
    "    print(\"----------------------------------------SCHEME:Double Nomralization-------------------------------------------\")\n",
    "    for i in sorted_tfidf_score_doublenorm:\n",
    "        print(\"Document: \", i , \", Tfidf score: \",sorted_tfidf_score_doublenorm[i])  \n",
    "        ctr += 1\n",
    "        if ctr == 5:\n",
    "            break \n",
    "        \n",
    "postinglist_binary = {}\n",
    "postinglist_raw = {}\n",
    "postinglist_termfreq = {}\n",
    "postinglist_lognorm = {}\n",
    "postinglist_doublenorm = {}\n",
    "matrix_binary = {}\n",
    "matrix_binary = get_matrix()  \n",
    "binary_tfidf()\n",
    "matrix_raw = {}\n",
    "matrix_raw= get_matrix() \n",
    "rawcount_tfidf()\n",
    "matrix_termfreq = {}\n",
    "matrix_termfreq= get_matrix() \n",
    "termfreq_tfidf()\n",
    "matrix_lognorm = {}\n",
    "matrix_lognorm= get_matrix() \n",
    "lognorm_tfidf()\n",
    "matrix_doublenorm = {}\n",
    "matrix_doublenorm= get_matrix()\n",
    "double_lognorm_tfidf()\n",
    "query_tokens = \"Calculate the experimental evaluation of theory of investigation to increase and make the maximum amount of approximations \"\n",
    "if query_tokens == \"\":\n",
    "    print(\"Query is empty\")\n",
    "else:\n",
    "    query_list = pre_process_text(query_tokens)\n",
    "    tfidf_score_binary(query_list)\n",
    "    tfidf_score_raw(query_list)\n",
    "    tfidf_score_termfreq(query_list)\n",
    "    tfidf_score_lognorm(query_list)\n",
    "    tfidf_score_doublenorm(query_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 1045 , Jaccard coeffecient: 0.09523809523809523\n",
      "Document: 0339 , Jaccard coeffecient: 0.08823529411764706\n",
      "Document: 0932 , Jaccard coeffecient: 0.08571428571428572\n",
      "Document: 0176 , Jaccard coeffecient: 0.08163265306122448\n",
      "Document: 0001 , Jaccard coeffecient: 0.07692307692307693\n",
      "Document: 0137 , Jaccard coeffecient: 0.06896551724137931\n",
      "Document: 0019 , Jaccard coeffecient: 0.06818181818181818\n",
      "Document: 0250 , Jaccard coeffecient: 0.06666666666666667\n",
      "Document: 0920 , Jaccard coeffecient: 0.06666666666666667\n",
      "Document: 0251 , Jaccard coeffecient: 0.06521739130434782\n"
     ]
    }
   ],
   "source": [
    "#JACCARD COEFFICIENT\n",
    "query_tokens = \"Calculate the experimental evaluation of theory of investigation to increase and make the maximum amount of approximations \"\n",
    "query_list = pre_process_text(query_tokens)\n",
    "Jaccard_coeff = {}\n",
    "for i in range(1,1401):\n",
    "    file = open(\"CSE508_Winter2023_Dataset/cranfield\"+getNum(i),\"r\")\n",
    "    file_content = file.read()\n",
    "    file.close()\n",
    "    extracted_content = pre_process_text(file_content, remove_duplicates=False)\n",
    "    query_list_set = set(query_list)\n",
    "    extracted_content_set = set(extracted_content) \n",
    "    intersection_doc_query = query_list_set.intersection(extracted_content_set) \n",
    "    union_doc_query = query_list_set.union(extracted_content_set)\n",
    "    jc = len(intersection_doc_query)/len(union_doc_query)\n",
    "    Jaccard_coeff[getNum(i)] = jc\n",
    "Jaccard_coeff_sort= dict(sorted(Jaccard_coeff.items(), key=operator.itemgetter(1),reverse=True))\n",
    "ctr = 0\n",
    "for i in Jaccard_coeff_sort:\n",
    "    print(\"Document:\",i,\", Jaccard coeffecient:\",Jaccard_coeff_sort[i])\n",
    "    ctr +=1\n",
    "    if(ctr == 10):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
