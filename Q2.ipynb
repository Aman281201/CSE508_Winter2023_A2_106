{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data For Test = 0.1 and Train = 0.9 :\n",
      "Using TFICF:\n",
      "Accuracy:   93.289\n",
      "Recall:   0.933\n",
      "Precision:   0.933\n",
      "F1 Score:   0.933\n",
      "\n",
      "Using TFIDF Vectorizer:\n",
      "Accuracy:   95.973\n",
      "Recall:   0.960\n",
      "Precision:   0.960\n",
      "F1 Score:   0.960\n",
      "\n",
      "Using n-gram(Unigram):\n",
      "Accuracy:   95.973\n",
      "Recall:   0.960\n",
      "Precision:   0.960\n",
      "F1 Score:   0.960\n",
      "\n",
      "Using n-gram(Bigram):\n",
      "Accuracy:   95.302\n",
      "Recall:   0.953\n",
      "Precision:   0.953\n",
      "F1 Score:   0.953\n",
      "\n",
      "Using n-gram(Unigram and Bigram):\n",
      "Accuracy:   95.302\n",
      "Recall:   0.953\n",
      "Precision:   0.953\n",
      "F1 Score:   0.953\n",
      "---------------------------------------------\n",
      "Data For Test = 0.2 and Train = 0.8 :\n",
      "Using TFICF:\n",
      "Accuracy:   95.638\n",
      "Recall:   0.956\n",
      "Precision:   0.956\n",
      "F1 Score:   0.956\n",
      "\n",
      "Using TFIDF Vectorizer:\n",
      "Accuracy:   96.644\n",
      "Recall:   0.966\n",
      "Precision:   0.966\n",
      "F1 Score:   0.966\n",
      "\n",
      "Using n-gram(Unigram):\n",
      "Accuracy:   97.315\n",
      "Recall:   0.973\n",
      "Precision:   0.973\n",
      "F1 Score:   0.973\n",
      "\n",
      "Using n-gram(Bigram):\n",
      "Accuracy:   96.980\n",
      "Recall:   0.970\n",
      "Precision:   0.970\n",
      "F1 Score:   0.970\n",
      "\n",
      "Using n-gram(Unigram and Bigram):\n",
      "Accuracy:   96.644\n",
      "Recall:   0.966\n",
      "Precision:   0.966\n",
      "F1 Score:   0.966\n",
      "---------------------------------------------\n",
      "Data For Test = 0.3 and Train = 0.7 :\n",
      "Using TFICF:\n",
      "Accuracy:   95.749\n",
      "Recall:   0.957\n",
      "Precision:   0.957\n",
      "F1 Score:   0.957\n",
      "\n",
      "Using TFIDF Vectorizer:\n",
      "Accuracy:   96.197\n",
      "Recall:   0.962\n",
      "Precision:   0.962\n",
      "F1 Score:   0.962\n",
      "\n",
      "Using n-gram(Unigram):\n",
      "Accuracy:   97.763\n",
      "Recall:   0.978\n",
      "Precision:   0.978\n",
      "F1 Score:   0.978\n",
      "\n",
      "Using n-gram(Bigram):\n",
      "Accuracy:   96.644\n",
      "Recall:   0.966\n",
      "Precision:   0.966\n",
      "F1 Score:   0.966\n",
      "\n",
      "Using n-gram(Unigram and Bigram):\n",
      "Accuracy:   96.868\n",
      "Recall:   0.969\n",
      "Precision:   0.969\n",
      "F1 Score:   0.969\n",
      "---------------------------------------------\n",
      "Data For Test = 0.4 and Train = 0.6 :\n",
      "Using TFICF:\n",
      "Accuracy:   94.631\n",
      "Recall:   0.946\n",
      "Precision:   0.946\n",
      "F1 Score:   0.946\n",
      "\n",
      "Using TFIDF Vectorizer:\n",
      "Accuracy:   96.141\n",
      "Recall:   0.961\n",
      "Precision:   0.961\n",
      "F1 Score:   0.961\n",
      "\n",
      "Using n-gram(Unigram):\n",
      "Accuracy:   96.812\n",
      "Recall:   0.968\n",
      "Precision:   0.968\n",
      "F1 Score:   0.968\n",
      "\n",
      "Using n-gram(Bigram):\n",
      "Accuracy:   95.805\n",
      "Recall:   0.958\n",
      "Precision:   0.958\n",
      "F1 Score:   0.958\n",
      "\n",
      "Using n-gram(Unigram and Bigram):\n",
      "Accuracy:   96.477\n",
      "Recall:   0.965\n",
      "Precision:   0.965\n",
      "F1 Score:   0.965\n",
      "---------------------------------------------\n",
      "Data For Test = 0.5 and Train = 0.5 :\n",
      "Using TFICF:\n",
      "Accuracy:   94.362\n",
      "Recall:   0.944\n",
      "Precision:   0.944\n",
      "F1 Score:   0.944\n",
      "\n",
      "Using TFIDF Vectorizer:\n",
      "Accuracy:   95.570\n",
      "Recall:   0.956\n",
      "Precision:   0.956\n",
      "F1 Score:   0.956\n",
      "\n",
      "Using n-gram(Unigram):\n",
      "Accuracy:   96.376\n",
      "Recall:   0.964\n",
      "Precision:   0.964\n",
      "F1 Score:   0.964\n",
      "\n",
      "Using n-gram(Bigram):\n",
      "Accuracy:   95.570\n",
      "Recall:   0.956\n",
      "Precision:   0.956\n",
      "F1 Score:   0.956\n",
      "\n",
      "Using n-gram(Unigram and Bigram):\n",
      "Accuracy:   95.973\n",
      "Recall:   0.960\n",
      "Precision:   0.960\n",
      "F1 Score:   0.960\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\",quiet=True)\n",
    "nltk.download(\"omw-1.4\",quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download(\"stopwords\",quiet=True)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class NaiveBayesImplementation():\n",
    "    vocab = {}\n",
    "    token_class_matrix = {}\n",
    "    naive_bayes_classifier = -1\n",
    "    classes = {}\n",
    "    token_class_matrix_org = {}\n",
    "    vectorizer = -1\n",
    "    processing_technique = -1\n",
    "    word_count_mat = -1\n",
    "    class_prior_prob = {}\n",
    "\n",
    "    def calculate_prior(self,data_y):\n",
    "        classes = sorted(list(data_y.unique()))\n",
    "        prior = {}\n",
    "        for i in classes:\n",
    "            prior[i] = (len(data_y[data_y==i])/len(data_y))\n",
    "        self.class_prior_prob = prior\n",
    "        return prior\n",
    "    \n",
    "    def prob_token_given_class(self,token,cl):\n",
    "        prob_word = self.word_count_mat[token]/self.word_count_mat[\"tot_words\"]\n",
    "        prob_class = self.class_prior_prob[cl]\n",
    "        tf_val = self.token_class_matrix_org[token][cl]\n",
    "        icf_val = np.log10((len(self.classes)+1)/(self.token_class_matrix_org[token][\"freq\"]+1))\n",
    "        numerator = tf_val * icf_val\n",
    "        denominator = 0\n",
    "        for i in self.classes:\n",
    "            tf_val = self.token_class_matrix_org[token][i]\n",
    "            icf_val = np.log10((len(self.classes)+1)/(self.token_class_matrix_org[token][\"freq\"]+1))\n",
    "            tf_icf_val = tf_val * icf_val\n",
    "            denominator = denominator + tf_icf_val\n",
    "        prob_class_given_token = numerator/denominator\n",
    "        fin_prob_token_given_class = (prob_class_given_token * prob_word)/(prob_class)\n",
    "        return fin_prob_token_given_class\n",
    "\n",
    "\n",
    "    def get_token_class_matrix(self,data_X, data_y): # matrix with row as token and column as class, stores count of token in the class\n",
    "        classes = data_y.unique()\n",
    "        new_dict = {}\n",
    "        for i in classes:\n",
    "            new_dict[i] = 0\n",
    "        mat = {}\n",
    "        for i in range(len(data_X)):\n",
    "            article = data_X.iloc[i]\n",
    "            this_class = data_y.iloc[i]\n",
    "            for token in article.split():\n",
    "                if token not in mat:\n",
    "                    mat[token] = new_dict.copy()\n",
    "                mat[token][this_class] += 1\n",
    "        for i in range(len(data_X)):\n",
    "            article = data_X.iloc[i]\n",
    "            this_class = data_y.iloc[i]\n",
    "            for token in article.split():\n",
    "                if(\"freq\" not in mat[token]):\n",
    "                    freq = 0\n",
    "                    for j in mat[token]:\n",
    "                        if(mat[token][j] > 0):\n",
    "                            freq+=1\n",
    "                    mat[token][\"freq\"] = freq\n",
    "        return mat\n",
    "\n",
    "    def get_vocabulary(self,data_X):\n",
    "        vocabulary = {}\n",
    "        self.word_count_mat = {}\n",
    "        k = 0\n",
    "        tot_words = 0\n",
    "        for i in range(len(data_X)):\n",
    "            article = data_X.iloc[i]\n",
    "            tokens = article.split()\n",
    "            for token in tokens:\n",
    "                if(token not in vocabulary):\n",
    "                    vocabulary[token] = k\n",
    "                    k+=1\n",
    "                if(token not in self.word_count_mat):\n",
    "                    self.word_count_mat[token] = 1\n",
    "                else:\n",
    "                    self.word_count_mat[token]+=1\n",
    "                tot_words+=1\n",
    "        self.word_count_mat[\"tot_words\"] = tot_words\n",
    "        return vocabulary\n",
    "\n",
    "    def get_token_class_tficf_matrix(self,data_X, data_y): # matrix with row as token and column as class, stores tficf value of token for each class\n",
    "        self.token_class_matrix_org = self.get_token_class_matrix(data_X,data_y)\n",
    "        self.classes = {}\n",
    "        id_val = 0\n",
    "        for cl in data_y.unique():\n",
    "            self.classes[cl] = id_val\n",
    "            id_val+=1 \n",
    "        self.vocab = self.get_vocabulary(data_X)\n",
    "        mat = []\n",
    "        for i in range(len(data_X)):\n",
    "            article = data_X.iloc[i]\n",
    "            article_mat = []\n",
    "            for cl in self.classes:\n",
    "                class_vec = [0] * len(self.vocab)\n",
    "                for token in article.split():\n",
    "                    token_id = self.vocab[token]\n",
    "                    tf_val = self.token_class_matrix_org[token][cl]\n",
    "                    icf_val = np.log10((len(self.classes)+1)/(self.token_class_matrix_org[token][\"freq\"]+1))\n",
    "                    tf_icf_val = tf_val * icf_val\n",
    "                    class_vec[token_id] = tf_icf_val\n",
    "                article_mat.append(class_vec)\n",
    "            mat.append(article_mat)\n",
    "        return mat\n",
    "    \n",
    "    def fit(self,data_X,data_y,model = \"Multinomial\", processing_technique = \"tficf\",ngram = (1,1)):\n",
    "        self.processing_technique = processing_technique\n",
    "        if(processing_technique == \"tficf\"):\n",
    "            self.token_class_matrix = self.get_token_class_tficf_matrix(data_X,data_y)\n",
    "            new_arr = np.array(self.token_class_matrix)\n",
    "            classifier_arr = []\n",
    "            for i in new_arr:\n",
    "                classifier_arr.append(i.flatten())\n",
    "            classifier_arr = np.array(classifier_arr)\n",
    "        elif(processing_technique == \"tfidf\"):\n",
    "            cv=TfidfVectorizer() \n",
    "            classifier_arr = cv.fit_transform(data_X)\n",
    "            self.vectorizer = cv\n",
    "        else:\n",
    "            cv=CountVectorizer(ngram_range = ngram) \n",
    "            classifier_arr = cv.fit_transform(data_X)\n",
    "            self.vectorizer = cv\n",
    "        if(model == \"Gaussian\"):\n",
    "            self.naive_bayes_classifier = GaussianNB()\n",
    "        elif(model == \"Multinomial\"):\n",
    "            self.naive_bayes_classifier = MultinomialNB()\n",
    "        self.naive_bayes_classifier.fit(classifier_arr, data_y)\n",
    "\n",
    "    def predict(self,data_X):\n",
    "        if(self.processing_technique == \"tficf\"):\n",
    "            mat = []\n",
    "            for i in range(len(data_X)):\n",
    "                article = data_X.iloc[i]\n",
    "                article_mat = []\n",
    "                for cl in self.classes:\n",
    "                    class_vec = [0] * len(self.vocab)\n",
    "                    for token in article.split():\n",
    "                        if(token in self.vocab):\n",
    "                            token_id = self.vocab[token]\n",
    "                            tf_val = self.token_class_matrix_org[token][cl]\n",
    "                            icf_val = np.log10((len(self.classes)+1)/(self.token_class_matrix_org[token][\"freq\"]+1))\n",
    "                            tf_icf_val = tf_val * icf_val\n",
    "                            class_vec[token_id] = tf_icf_val\n",
    "                    article_mat.append(class_vec)\n",
    "                mat.append(article_mat)\n",
    "            mat = np.array(mat)\n",
    "            classifier_arr = []\n",
    "            for i in mat:\n",
    "                classifier_arr.append(i.flatten())\n",
    "            classifier_arr = np.array(classifier_arr)\n",
    "        elif(self.processing_technique== \"tfidf\"):\n",
    "            classifier_arr = self.vectorizer.transform(data_X)\n",
    "        else:\n",
    "            classifier_arr = self.vectorizer.transform(data_X)\n",
    "\n",
    "        pred = self.naive_bayes_classifier.predict(classifier_arr) \n",
    "        return pred\n",
    "    \n",
    "    def show_metrics(self,y_test,y_pred):\n",
    "        score1 = metrics.accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:   %0.3f\" % (score1*100))\n",
    "\n",
    "        score2 = metrics.recall_score(y_test, y_pred, average = \"micro\")\n",
    "        print(\"Recall:   %0.3f\" % (score2))\n",
    "\n",
    "        score3 = metrics.precision_score(y_test, y_pred, average = \"micro\")\n",
    "        print(\"Precision:   %0.3f\" % (score3))\n",
    "\n",
    "        score4 = metrics.f1_score(y_test, y_pred, average = \"micro\")\n",
    "        print(\"F1 Score:   %0.3f\" % (score4))\n",
    "\n",
    "def preProcessData(dataset):\n",
    "    data_X = dataset.copy()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for i in range(len(data_X)):\n",
    "        article = data_X[i]\n",
    "        new_s = article.lower() # data converted to lower case\n",
    "        translate_table = dict((ord(char), \" \") for char in string.punctuation)   \n",
    "        new_s = new_s.translate(translate_table) # punctuations removed from data\n",
    "        li = word_tokenize(new_s) # words tokenized \n",
    "        stop_words = set(stopwords.words(\"english\")) # stop words identified\n",
    "        filter_li = []\n",
    "        for words in li:\n",
    "            if(words not in stop_words):\n",
    "                filter_li.append(words) # stop words removed\n",
    "        for j in range(len(filter_li)):\n",
    "            filter_li[j] = wnl.lemmatize(filter_li[j], pos=\"v\") # every token lemmatized to find root form \n",
    "        processed_article =  \" \".join(filter_li) # tokens joined back with a space \n",
    "        data_X[i] = processed_article #new string written back to dataset object\n",
    "    return data_X\n",
    "\n",
    "def read_dataset(path = 'Q2_Dataset/BBC News Train.csv', preprocess = True):\n",
    "    dataset = pd.read_csv(path) \n",
    "    dataset.drop(columns=[\"ArticleId\"])\n",
    "    data_X = dataset['Text'] \n",
    "    data_y = dataset['Category']\n",
    "    if(preprocess):\n",
    "        data_X = preProcessData(dataset = data_X)\n",
    "    return data_X,data_y\n",
    "\n",
    "def check_test_train(data_X, data_y, test = 0.30, train = 0.70):\n",
    "    if(train + test > 1):\n",
    "        print(\"Invalid Split\")\n",
    "        return -1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=test,train_size=train,random_state=0)\n",
    "    \n",
    "    print(\"Using TFICF:\")\n",
    "    nb = NaiveBayesImplementation()\n",
    "    nb.fit(X_train,y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    nb.show_metrics(y_test,y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\"Using TFIDF Vectorizer:\")\n",
    "    nb1 = NaiveBayesImplementation()\n",
    "    nb1.fit(X_train,y_train,processing_technique=\"tfidf\")\n",
    "    y_pred = nb1.predict(X_test)\n",
    "    nb1.show_metrics(y_test,y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\"Using n-gram(Unigram):\")\n",
    "    nb2 = NaiveBayesImplementation()\n",
    "    nb2.fit(X_train,y_train,processing_technique=\"count_vec\")\n",
    "    y_pred = nb2.predict(X_test)\n",
    "    nb2.show_metrics(y_test,y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\"Using n-gram(Bigram):\")\n",
    "    nb3 = NaiveBayesImplementation()\n",
    "    nb3.fit(X_train,y_train,processing_technique=\"count_vec\",ngram=(2,2))\n",
    "    y_pred = nb3.predict(X_test)\n",
    "    nb3.show_metrics(y_test,y_pred)\n",
    "    print()\n",
    "\n",
    "    print(\"Using n-gram(Unigram and Bigram):\")\n",
    "    nb4 = NaiveBayesImplementation()\n",
    "    nb4.fit(X_train,y_train,processing_technique=\"count_vec\",ngram=(1,2))\n",
    "    y_pred = nb4.predict(X_test)\n",
    "    nb4.show_metrics(y_test,y_pred)\n",
    "\n",
    "data_X, data_y = read_dataset()\n",
    "splits = [(),(),(),(),()]\n",
    "for i in range(len(splits)):\n",
    "    test_split = (i+1)/10\n",
    "    splits[i] = (test_split, 1 - test_split)\n",
    "\n",
    "for i in splits:\n",
    "    print(\"Data For Test =\",i[0],\"and Train =\",i[1],\":\")\n",
    "    check_test_train(data_X,data_y,i[0],i[1])\n",
    "    print(\"---------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
